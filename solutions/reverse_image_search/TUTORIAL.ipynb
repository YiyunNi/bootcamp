{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a213cfee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reverse image search\n",
    "In this example we will be going over the code required to perform reverse image searches. This example uses a ResNet model to extract image features that are then used with Milvus to build a system that can perform the searches. \n",
    "## Data\n",
    "\n",
    "This example uses the PASCAL VOC image set, which contains 17125 images within 20 categories: human; animals (birds, cats, cows, dogs, horses, sheep); transportation (planes, bikes,boats, buses, cars, motorcycles, trains); household (bottles, chairs, tables, pot plants, sofas, TVs)\n",
    "\n",
    "Dataset size: ~ 2 GB.\n",
    "\n",
    "Directory Structure:  \n",
    "The file loader used in this requires that the folders containing the images are subfolders. \n",
    "\n",
    "```bash\n",
    "__data_directory  \n",
    "    |__sub_folder_1  \n",
    "    |   |__image1.jpg  \n",
    "    |   |__image2.jpg  \n",
    "    |__sub_folder_2  \n",
    "        |__imageX.jpg  \n",
    "```\n",
    "\n",
    "> Note: You can also use other images for testing. This example only requires that the images are PIL compatible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8102f770",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "| Python Packages | Docker Servers |\n",
    "| --------------- | -------------- |\n",
    "| PyMilvus        | Milvus-1.1.0   |\n",
    "| Redis           | Redis          |\n",
    "| PyTorch |\n",
    "| TorchVision |\n",
    "| MatPlotLib |\n",
    "| PIL |\n",
    "| Numpy |\n",
    "\n",
    "For this example we are assuming you are familiar with using Pytorch, MatPlotLib, and TorchVision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d0fe1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Up and Running\n",
    "\n",
    "### Start Milvus Server\n",
    "\n",
    "This demo uses Milvus 1.1.0, please refer to the [Install Milvus](https://milvus.io/docs/v1.1.0/install_milvus.md) guide to learn how to use this docker container. For this example we wont be mapping any local volumes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "937cf07b-726e-49f6-af41-2ef5cd8db36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8147263897dd53845cbf7e64f03f4586ed720979f3f3d6811ced0d3de07b9b30\n",
      "docker: Error response from daemon: driver failed programming external connectivity on endpoint boring_chatelet (f22bb9db6804467c6d67dee03edc479ff66acabea0e9ca4b8ad59c655cb4f1a5): Bind for 0.0.0.0:19530 failed: port is already allocated.\n"
     ]
    }
   ],
   "source": [
    "! docker run -d \\\n",
    "-p 19530:19530 \\\n",
    "-p 19121:19121 \\\n",
    "milvusdb/milvus:1.1.0-cpu-d050721-5e559c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d9cc3-d456-42db-8c9f-0372eb11f5f9",
   "metadata": {},
   "source": [
    "### Start Redis Server\n",
    "We are using Redis as a metadata storage service. Code can easily be modified to use python dictionary, but that usually does not work in any use case outside of quick examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de1ecc1-d28f-4944-9b46-7a478d653f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -d -p 6379:6379 redis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047b80af",
   "metadata": {},
   "source": [
    "## Code Overview\n",
    "### Downloading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d30b76-6002-45c5-8a11-6ca79e77f4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file1.tar            89%[================>   ]   1.66G  5.81MB/s    eta 39s    "
     ]
    }
   ],
   "source": [
    "! wget -O 'file1.tar' 'http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar' -q --show-progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c366909-8d7c-47d0-9057-82b87484463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf 'file1.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e249084-6e5f-4a27-b980-c7ea436fdbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r -f ./VOCdevkit/VOC2012/Annotations\n",
    "! rm -r -f ./VOCdevkit/VOC2012/ImageSets\n",
    "! rm -r -f ./VOCdevkit/VOC2012/Annotations\n",
    "! rm -r -f ./VOCdevkit/VOC2012/SegmentationClass\n",
    "! rm -r -f ./VOCdevkit/VOC2012/SegmentationObject\n",
    "! rm -f 'file1.tar'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0749aca7-df6e-4897-b65b-72eb69cb2c6c",
   "metadata": {},
   "source": [
    "### Connecting to Servers\n",
    "\n",
    "We first start off by connecting to the servers. In this case the docker containers are running on localhost and the ports are the default ports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d15825-1a18-45d0-9b11-559886c2e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connectings to Milvus and Redis\n",
    "\n",
    "import redis\n",
    "import milvus\n",
    "\n",
    "milv = milvus.Milvus(host = '127.0.0.1', port = 19530)\n",
    "red = redis.Redis(host = '127.0.0.1', port=6379, db=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85801f40",
   "metadata": {},
   "source": [
    "### Building Collection and Setting Index\n",
    "\n",
    "The next step involves creating a collection. A collection in Milvus is similar to a table in a relational database, and is used for storing all the vectors. To create a collection, we first must select a name, the dimension of the vectors being stored within, the index_file_size, and metric_type. The index_file_size corresponds to how large each data segmet will be within the collection. More information on this can be found here. The metric_type is the distance formula being used to calculate similarity. In this example we are using the Euclidean distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db730ba7-618e-40c4-9f9f-a45e7dbf3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating collection\n",
    "\n",
    "import time\n",
    "\n",
    "collection_name = \"test_collection\"\n",
    "milv.drop_collection(collection_name) \n",
    "red.flushdb()\n",
    "time.sleep(.1)\n",
    "\n",
    "collection_param = {\n",
    "            'collection_name': collection_name,\n",
    "            'dimension': 512,\n",
    "            'index_file_size': 1024,  # optional\n",
    "            'metric_type': milvus.MetricType.L2  # optional\n",
    "            }\n",
    "\n",
    "status, ok = milv.has_collection(collection_name)\n",
    "\n",
    "if not ok:\n",
    "    status = milv.create_collection(collection_param)\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8cf359",
   "metadata": {},
   "source": [
    "After creating the collection we want to assign it an index type. This can be done before or after inserting the data. When done before, indexes will be made as data comes in and fills the data segments. In this example we are using IVF_SQ8 which requires the 'nlist' parameter. Each index types carries its own parameters. More info about this param can be found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe53c8-4752-4872-914f-4d49096992d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing collection\n",
    "\n",
    "index_param = {\n",
    "    'nlist': 512\n",
    "}\n",
    "\n",
    "status = milv.create_index(collection_name, milvus.IndexType.IVF_SQ8, index_param)\n",
    "status, index = milv.get_index_info(collection_name)\n",
    "print(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36cdc6",
   "metadata": {},
   "source": [
    "### Processing and Storing Images\n",
    "\n",
    "In order to store the images in Milvus, we must first run them through the ResNet model. In this case, we are using the pretrained ResNet-18 model provided by PyTorch. In order to get the feature vectors, we must remove the classifying layer that comes at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24664b23-94df-41ab-ab7a-73c12ba75d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/filiphaltmayer/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    }
   ],
   "source": [
    "# Preparing model and dataloader\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.9.0', 'resnet18', pretrained=True)\n",
    "encoder = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "encoder.eval()\n",
    "\n",
    "data_dir = \"./VOCdevkit\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928a3567",
   "metadata": {},
   "source": [
    "In this example we are also using a slightly modified dataloader that also returns the file path of the image. With this dataloader we are also transforming the images into what ResNet model takes as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a689e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        return super(ImageFolderWithPaths, self).__getitem__(index) + (self.imgs[index][0],)\n",
    "\n",
    "dataset = ImageFolderWithPaths(data_dir, transform=transforms.Compose([\n",
    "                                                transforms.Resize(256),\n",
    "                                                transforms.CenterCrop(224),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, num_workers=0, batch_size = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005c8168",
   "metadata": {},
   "source": [
    "Inputting the data involves three major steps. First, the images need to be run through the model. This outputs vectors for each image. Second, these vectors are pushed into Milvus. Milvus then returns the corresponding IDs for the vectors. Third, these IDs and the image filepaths are used as the key and value for storage in Redis. Redis is used so that we can return the original image as a result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f2ac471-f13f-4bad-8077-f85256b6cfaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'milv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d0887cae5ab1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmilv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollection_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'milv' is not defined"
     ]
    }
   ],
   "source": [
    "# Processing and storing\n",
    "\n",
    "for inputs, labels, paths in dataloader:\n",
    "    with torch.no_grad():\n",
    "        output = encoder(inputs).squeeze()\n",
    "        output = output.numpy()\n",
    "\n",
    "    status, ids = milv.insert(collection_name=collection_name, records=output)\n",
    "\n",
    "    if not status.OK():\n",
    "        print(\"Insert failed: {}\".format(status))\n",
    "    else:\n",
    "        for x in range(len(ids)):\n",
    "            red.set(str(ids[x]), paths[x])\n",
    "        print(\"Added: \" + str(len(ids)) + \" vectors into: \" + collection_name + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fc8e22-8f5b-4bfc-96e0-22eafe0bc0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper display function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def show_results(query, results, distances):\n",
    "    \n",
    "    fig_query, ax_query = plt.subplots(1,1, figsize=(5,5))\n",
    "    ax_query.imshow(Image.open(query))\n",
    "    ax_query.axis('off')\n",
    "    ax_query.set_title(\"Searched Image\")\n",
    "    \n",
    "    res_count = len(results)\n",
    "    fig, ax = plt.subplots(1,res_count,figsize=(5,5))\n",
    "    for x in range(res_count):\n",
    "        ax[x].imshow(Image.open(results[x]))\n",
    "        ax[x].axis('off')\n",
    "        dist =  str(distances[x])\n",
    "        dist = dist[0:dist.find('.')+4]\n",
    "        ax[x].set_title(\"D: \" +dist)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b7d637-73ca-4928-afa4-eeaf12d2ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling out random images to search\n",
    "\n",
    "random_ids = [int(red.randomkey()) for x in range(10)]\n",
    "search_images = [x.decode(\"utf-8\") for x in red.mget(random_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b62bcbc",
   "metadata": {},
   "source": [
    "### Searching\n",
    "\n",
    "When searching for an image, we first put the image through the same transformations as the ones used for storing the images. Once transformed, we run the image through the ResNet to get the corresponding vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d58d45-775e-453d-a4e2-80ebfefda180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing query images and searching\n",
    "\n",
    "transform_ops = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    \n",
    "embeddings = [transform_ops(Image.open(x)) for x in search_images]\n",
    "embeddings = torch.stack(embeddings, dim=0)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    embeddings = encoder(embeddings).squeeze().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68544d93",
   "metadata": {},
   "source": [
    "Then we can use these embeddings in a search. The search requires a few arguments. It needs the name of the collection, the vectors being searched for, how many closest vectors to be returned, and the parameters for the index, in this case nprobe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22c1158",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_sub_param = {\n",
    "        \"nprobe\": 16\n",
    "    }\n",
    "\n",
    "search_param = {\n",
    "    'collection_name': collection_name,\n",
    "    'query_records': embeddings,\n",
    "    'top_k': 3,\n",
    "    'params': search_sub_param,\n",
    "    }\n",
    "\n",
    "start = time.time()\n",
    "status, results = milv.search(**search_param)\n",
    "end = time.time() - start\n",
    "\n",
    "print(\"Search took a total of: \", end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b735af",
   "metadata": {},
   "source": [
    "The result of this search contains the IDs and corresponding distances of the top_k closes vectors. We can use the IDs in Redis to get the original image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf2e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if status.OK():\n",
    "    for x in range(len(results)):\n",
    "        query_file = search_images[x]\n",
    "        result_files = [red.get(y.id).decode('utf-8') for y in results[x]]\n",
    "        distances = [y.distance for y in results[x]]\n",
    "        show_results(query_file, result_files, distances)\n",
    "else:\n",
    "    print(\"Search Failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1ac1c6",
   "metadata": {},
   "source": [
    "This is the basic way to do a reverse image search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
